{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09.decision_trees",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changsin/MIU_ML/blob/main/notebooks/09.decision_trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HclJutxS2bYj"
      },
      "source": [
        "# Decision Tree\n",
        "A decision tree is a hierarchical data structure implementing the\n",
        "divide-and-conquer strategy based on test decisions. At each node, a test is performed to determine which branch to take. Once the branch is decided, another test is performed to repeat the process until it reaches a leaf node. It is an efficient nonparametric method, which can be used for both classification and regression. (p. 213, Alpayden 2016)\n",
        "\n",
        "Not all decision trees are the same. Some trees are more efficient than others. A shorter tree means that there are less decisions to make to reach a leaf node so it is more efficient. The goal of the algorithm is to build the shortest and thus most efficient decision tree. To compare the effectiveness of decision trees, information gain or uncertainty metrics are used. To calculate uncertainty measures, Shanon's entropy formula is used. The converse of entropy is information gain which is explained in the next section.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oFEJiGK0ZwY"
      },
      "source": [
        "## Entropy\n",
        "\n",
        "The entropy is calculated according to Shannon's formula:\n",
        "\n",
        "$ H(p) = H(p_1;...p_n) = -\\Sigma_{n=1}^{n}p_i log_2 p_i = H(D) $\n",
        "\n",
        "Applied to the skiing example in the text book, we calculate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuQKJ69uQdqN"
      },
      "source": [
        "### Example 1 - Skiing\n",
        "The following is an explanation of Decision Tree (section 8.4, pp. 200~) in Ertel's Artificial Intelligence. The data used is reproduced below. The last column _Skiing_ represents the target decision of either going or not going to skiing.\n",
        "\n",
        "\n",
        "| Day | Snow_Dist | Weekend | Sun | Skiing |\n",
        "| ----| --------|-----------|------| ------|\n",
        "| 1 | $ \\leq $100 | yes | yes | yes |\n",
        "| 2 | $ \\leq $100 | yes | yes | yes |\n",
        "| 3 | $ \\leq $100 | yes | no | yes |\n",
        "| 4 | $ \\leq $100 | no | yes | yes |\n",
        "| 5 | >100 | yes | yes | yes |\n",
        "| 6 | >100 | yes | yes | yes |\n",
        "| 7 | >100 | yes | yes | no |\n",
        "| 8 | >100 | yes | no | no |\n",
        "| 9 | >100 | no | yes | no |\n",
        "| 10 | >100 | no | yes | no |\n",
        "| 11 | >100 | no | no | no |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hQBW-9ork94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2281f2-5449-4c1c-8710-756d866860a3"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "p1 = 6/11 # 6 yes' in skiing column\n",
        "p2 = 5/11 # 4 no's in skiing column\n",
        "p3 = 7/11\n",
        "\n",
        "probs = np.array([p1, p2])\n",
        "\n",
        "def calculate_entropy(probs):\n",
        "  entropy = 0\n",
        "  for p in probs:\n",
        "    if p != 0:\n",
        "      entropy += p*np.log2(p)\n",
        "  \n",
        "  return -entropy\n",
        "\n",
        "\n",
        "\n",
        "H = -(p1*np.log2(p1)) - (p2*np.log2(p2))\n",
        "H\n",
        "\n",
        "calculate_entropy(probs)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9940302114769565"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial entropy of Skiing: 6 yes' and 5 yes"
      ],
      "metadata": {
        "id": "goJlI_4iziRt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYI4IgstkfDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0087f39-d34b-499a-fbc6-f1400ef14971"
      },
      "source": [
        "calculate_entropy(np.array([6/11, 5/11]))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9940302114769565"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "entropy of >100 snow distribution"
      ],
      "metadata": {
        "id": "Vqg1IgIw4qCS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5ItFMaGdmjH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da4c7b0-96ee-488a-c228-d1227e846b99"
      },
      "source": [
        "calculate_entropy(np.array([2/7, 5/7]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.863120568566631"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "entropy of <= 100 snow distribution"
      ],
      "metadata": {
        "id": "oUBCcpuN4wVm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUMuUsBAj3HZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed702328-e52f-49ee-efb1-82415d685184"
      },
      "source": [
        "calculate_entropy(np.array([4/4, 0/4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEJK8GTX2FgC"
      },
      "source": [
        "## Information Gain\n",
        "\n",
        "For dataset D, the information gained is thus:\n",
        "\n",
        "$$ I(D) = 1 - H(D) $$\n",
        "\n",
        "The information gain through attribute A is defined as:\n",
        "$$ G(D, A) = \\Sigma_{i=1}^{n}\\frac{|D_i|}{|D|}I(D_i) - I(D) $$\n",
        "\n",
        "or if we rewrite it in terms of entropy:\n",
        "\n",
        "$$ = H(D) - \\Sigma_{i=1}^{n}\\frac{|D_i|}{|D|}H(D_i) $$\n",
        "\n",
        "Applied to our skiing example, for the choice of snow distribution, we get:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9zs5opWrKGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b594a7e-c536-4030-b30f-35f2ea517707"
      },
      "source": [
        "D = 11\n",
        "D_snow_little = 4\n",
        "D_snow_big = 7\n",
        "H_D = calculate_entropy(np.array([6/11, 5/11]))\n",
        "H_snow_little = calculate_entropy(np.array([4/4, 0/4]))\n",
        "H_snow_big = calculate_entropy(np.array([2/7, 5/7]))\n",
        "H_snow_big"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.863120568566631"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NuMkVGr14g4"
      },
      "source": [
        "0.863 is the entropy we get when we branch the decision tree for snow distribution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRc9s5NjlwhL"
      },
      "source": [
        "def calculate_information_gain(dataset_count, initial_entropy, subs):\n",
        "  \"\"\"\n",
        "    dataset_count: total number of entries in the dataset\n",
        "    initial_entropy: the initial entropy of the dataset\n",
        "    subs: a list of positive and negative counts for each sub-branch:\n",
        "      e.g., [[4, 0], [2, 5]] means that for snow_dist,\n",
        "       for the positive case, <= 100 give you 4 correct and 0 incorrect answers.\n",
        "       for the negative case, 2 incorrect and 5 correct answers.\n",
        "  \"\"\"\n",
        "  sub_entropy = 0\n",
        "\n",
        "  for sub in subs:\n",
        "    sub_counts = np.array(sub)\n",
        "    sub_total = np.sum(sub_counts)\n",
        "    sub_probs = np.array([ di/sub_total for di in sub_counts])\n",
        "\n",
        "    entropy = calculate_entropy(sub_probs)\n",
        "    print(\"subentropy\", entropy)\n",
        "\n",
        "    sub_entropy += (sub_total/dataset_count)*entropy\n",
        "\n",
        "  return initial_entropy - sub_entropy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SATgMWe71Tsj"
      },
      "source": [
        "## How to build a decision tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LipppCdD04YR"
      },
      "source": [
        "Using the information gain formula, we can now calculate and compare what the best way is to build a decision tree. Given there are three attributes, we calculate the information gain for each as follows.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqZzohnm1S7h"
      },
      "source": [
        "### A1: Snow distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2Oew6GM026M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da2335bc-3214-469e-e661-7a69b7d64d59"
      },
      "source": [
        "H_D = calculate_entropy(np.array([6/11, 5/11]))\n",
        "sub_snow = [[4, 0],\n",
        "            [5, 2]]\n",
        "calculate_information_gain(11, H_D, sub_snow)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subentropy -0.0\n",
            "subentropy 0.863120568566631\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.44477166784364586"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e546ow3h2cWo"
      },
      "source": [
        "### A2: Weekend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZZalp3mpazt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe20888-b63a-4d7b-e7a2-0681f26a2890"
      },
      "source": [
        "sub_weekend = [[5, 2],  # 5 correct, 2 incorrect\n",
        "               [1, 3]]  # 1 incorrect, 3 correct\n",
        "calculate_information_gain(11, H_D, sub_weekend)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subentropy 0.863120568566631\n",
            "subentropy 0.8112781244591328\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14976144076759756"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBZNWVgw2kaU"
      },
      "source": [
        "### A3: Sun"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7cryB1YreL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfeb16f-3af0-4ff9-9ba7-0128f164a1ba"
      },
      "source": [
        "sub_sun = [[5, 3],\n",
        "            [1, 2]]\n",
        "calculate_information_gain(11, H_D, sub_sun)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subentropy 0.9544340029249649\n",
            "subentropy 0.9182958340544896\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0494520727893939"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scJoerE52oLI"
      },
      "source": [
        "Based on the results, we know that snow distribution is the first branch question we should ask to get the most information gain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWF8YvMDQB1W"
      },
      "source": [
        "## Example 2\n",
        "\n",
        "\n",
        "We will use the dataset below to learn a decision tree which predicts if people pass machine\n",
        "learning (Yes or No), based on their previous GPA (High or Low) and whether or\n",
        "not they studied.\n",
        "\n",
        "| # | Previous GPA | Studied | Pass |\n",
        "| --- | --- | --- | --- |\n",
        "| 1 | $ \\leq $ 3.0 | No | No |\n",
        "| 2 | $ \\leq $ 3.0 | Yes | Yes |\n",
        "| 3 | > 3.0 | No | No |\n",
        "| 4 | > 3.0 | Yes | Yes |\n",
        "| 5 | > 3.0 | No | Yes |\n",
        "| 6 | > 3.0 | Yes | Yes |\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMK0BAqsQEQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0062039-52f9-494a-e3e4-d2ff3abd71f0"
      },
      "source": [
        "probs = [1/6, 4/6]\n",
        "HD_pass = calculate_entropy(probs)\n",
        "HD_pass"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8208020839342969"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_gpa = [[1, 1],\n",
        "            [1, 3]]\n",
        "calculate_information_gain(7, HD_pass, sub_gpa)"
      ],
      "metadata": {
        "id": "dylYn71r6pRy",
        "outputId": "5b5b2b52-bdf9-4ae1-d723-fd73eb179c64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subentropy 1.0\n",
            "subentropy 0.8112781244591328\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07150029852907813"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sub_studied = [[1, 2],\n",
        "            [3, 0]]\n",
        "calculate_information_gain(7, HD_pass, sub_gpa)"
      ],
      "metadata": {
        "id": "IIndEdF__tK6",
        "outputId": "8858d6b0-fca7-4465-8df0-2c0c2d820445",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subentropy 1.0\n",
            "subentropy 0.8112781244591328\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07150029852907813"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w3_VE6OAABNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}